{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPyK2G6yXKtFhFbWY5DxHGf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"tTGqXv1WIeKC","executionInfo":{"status":"ok","timestamp":1671603278766,"user_tz":480,"elapsed":3654,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}}},"outputs":[],"source":["# https://www.tensorflow.org/tutorials/text/word2vec\n","\n","import io\n","import re\n","import string\n","import tqdm\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers"]},{"cell_type":"code","source":["import nltk\n","nltk.download('gutenberg')\n","nltk.download('punkt')\n","from nltk.lm import Vocabulary\n","import string "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H9cIx3fyE0qU","executionInfo":{"status":"ok","timestamp":1671603283929,"user_tz":480,"elapsed":3093,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"737da411-1109-43ee-ad40-475704e44d54"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/gutenberg.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["SEED = 42\n","num_ns = 4 # number of negative samples\n","CHECKPOINT_PATH = '/content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt' # for ModelCheckpoint\n","log_dir = \"/content/gdrive/My Drive/Colab/logs/fit/word2vec\" # for tensorboard\n"],"metadata":{"id":"JyL6royQbKE6","executionInfo":{"status":"ok","timestamp":1671603286502,"user_tz":480,"elapsed":387,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["### enable google drive access\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hCDCEtvGbVN0","executionInfo":{"status":"ok","timestamp":1671603349568,"user_tz":480,"elapsed":21672,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"c380037e-edf0-42d9-8e0e-5252fb4c8f15"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["Preprocess text using nltk:"],"metadata":{"id":"8rd6w55dLgvQ"}},{"cell_type":"code","source":["words = nltk.corpus.gutenberg.words('austen-emma.txt')\n","sentences = nltk.corpus.gutenberg.sents('austen-emma.txt')\n","sentences[10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NDYoRAxoFDkx","executionInfo":{"status":"ok","timestamp":1671605481885,"user_tz":480,"elapsed":344,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"1297b2af-22bf-4311-f746-8a542719a9a9"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['The',\n"," 'danger',\n"," ',',\n"," 'however',\n"," ',',\n"," 'was',\n"," 'at',\n"," 'present',\n"," 'so',\n"," 'unperceived',\n"," ',',\n"," 'that',\n"," 'they',\n"," 'did',\n"," 'not',\n"," 'by',\n"," 'any',\n"," 'means',\n"," 'rank',\n"," 'as',\n"," 'misfortunes',\n"," 'with',\n"," 'her',\n"," '.']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["my_vocab, index = {}, 1  # start indexing from 1\n","my_vocab['<pad>'] = 0  # add a padding token\n","for w in words:\n","  w = w.lower()\n","  if w in string.punctuation: continue\n","\n","  if w not in my_vocab:\n","    my_vocab[w] = index\n","    index += 1\n","\n","my_inverse_vocab = {index: token for token, index in my_vocab.items()}"],"metadata":{"id":"wX7CPlISLv5P","executionInfo":{"status":"ok","timestamp":1671605485693,"user_tz":480,"elapsed":289,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["ind_sentences = []\n","for sentence in sentences:\n","  ind_sentence = []\n","  for w in sentence:\n","    w = w.lower()\n","    if w not in my_vocab: continue\n","\n","    ind_sentence.append(my_vocab[w])\n","  ind_sentences.append(ind_sentence)\n","    "],"metadata":{"id":"rlZOduL8LoW_","executionInfo":{"status":"ok","timestamp":1671605489155,"user_tz":480,"elapsed":1281,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["my_sent = ind_sentences[10]\n","[my_inverse_vocab[index] for index in my_sent]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kZ3rg_mGcNV","executionInfo":{"status":"ok","timestamp":1671605493742,"user_tz":480,"elapsed":305,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"f0bbc503-b54e-49be-9e7c-e17bce7b3a21"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the',\n"," 'danger',\n"," 'however',\n"," 'was',\n"," 'at',\n"," 'present',\n"," 'so',\n"," 'unperceived',\n"," 'that',\n"," 'they',\n"," 'did',\n"," 'not',\n"," 'by',\n"," 'any',\n"," 'means',\n"," 'rank',\n"," 'as',\n"," 'misfortunes',\n"," 'with',\n"," 'her']"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["--------------------------------------------------------------------------------"],"metadata":{"id":"b5A9DGL_LdY6"}},{"cell_type":"code","source":["sentence = \"The wide road shimmered in the hot sun\"\n","tokens = list(sentence.lower().split())\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOLDbZWfIofe","executionInfo":{"status":"ok","timestamp":1671476614522,"user_tz":480,"elapsed":172,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"7a8bec9d-7e70-42a6-b67b-a10a045ac04c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the', 'wide', 'road', 'shimmered', 'in', 'the', 'hot', 'sun']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["vocab, index = {}, 1  # start indexing from 1\n","vocab['<pad>'] = 0  # add a padding token\n","for token in tokens:\n","  if token not in vocab:\n","    vocab[token] = index\n","    index += 1\n","vocab_size = len(vocab)\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TV9sbLUKItfp","executionInfo":{"status":"ok","timestamp":1671476616485,"user_tz":480,"elapsed":173,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"b46838c7-474b-429f-8d07-d77e57ede2c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"]}]},{"cell_type":"code","source":["inverse_vocab = {index: token for token, index in vocab.items()}\n","print(inverse_vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAro9Bt1IxV4","executionInfo":{"status":"ok","timestamp":1671476618229,"user_tz":480,"elapsed":146,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"539fee02-ec60-4a30-c9ab-c64de0bf9ccb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"]}]},{"cell_type":"code","source":["example_sequence = [vocab[word] for word in tokens]\n","print(example_sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XP-uBZaZI3S_","executionInfo":{"status":"ok","timestamp":1671476620119,"user_tz":480,"elapsed":264,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"5a432fcb-d6d5-4ba8-d61a-ce7c27e1420f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5, 1, 6, 7]\n"]}]},{"cell_type":"code","source":["# Generates skip-gram pairs with negative sampling for a list of sequences\n","# (int-encoded sentences) based on window size, number of negative samples\n","# and vocabulary size.\n","def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n","  # Elements of each training example are appended to these lists.\n","  targets, contexts, labels = [], [], []\n","\n","  # Build the sampling table for `vocab_size` tokens.\n","  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n","\n","  # Iterate over all sequences (sentences) in the dataset.\n","  for sequence in tqdm.tqdm(sequences):\n","\n","    # Generate positive skip-gram pairs for a sequence (sentence).\n","    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","          sequence,\n","          vocabulary_size=vocab_size,\n","          window_size=window_size,\n","          negative_samples=0)\n","\n","    # Iterate over each positive skip-gram pair to produce training examples\n","    # with a positive context word and negative samples.\n","    for target_word, context_word in positive_skip_grams:\n","      context_class = tf.expand_dims(\n","          tf.constant([context_word], dtype=\"int64\"), 1)\n","      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","          true_classes=context_class,\n","          num_true=1,\n","          num_sampled=num_ns,\n","          unique=True,\n","          range_max=vocab_size,\n","          seed=seed,\n","          name=\"negative_sampling\")\n","\n","      # Build context and label vectors (for one target word)\n","      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n","      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n","\n","      # Append each element from the training example to global lists.\n","      targets.append(target_word)\n","      contexts.append(context)\n","      labels.append(label)\n","\n","  return targets, contexts, labels"],"metadata":{"id":"nMv_b80P_b-z","executionInfo":{"status":"ok","timestamp":1671566685739,"user_tz":480,"elapsed":132,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# example\n","#targets, contexts, labels = generate_training_data([example_sequence], window_size=2, num_ns=4, vocab_size=vocab_size, seed=SEED)\n","#targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6OI1uj4AWoR","executionInfo":{"status":"ok","timestamp":1671481368640,"user_tz":480,"elapsed":176,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"88e8b4ac-2ced-48a9-d244-c2c15e9321c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00, 112.98it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["[5, 5, 1, 5, 3, 4, 1, 1, 2, 4, 5, 1, 2, 3, 4, 2, 3, 6, 4, 7, 7, 6, 6, 1, 1, 3]"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# takes time\n","targets, contexts, labels = generate_training_data(ind_sentences, window_size=2, num_ns=4, vocab_size=len(my_vocab), seed=SEED)"],"metadata":{"id":"ZHgWqPQwEaxS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["targets = np.array(targets)\n","contexts = np.array(contexts)\n","labels = np.array(labels)\n","\n","print(f\"targets.shape: {targets.shape}\")\n","print(f\"contexts.shape: {contexts.shape}\")\n","print(f\"labels.shape: {labels.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"le8PDU_iJn3z","executionInfo":{"status":"ok","timestamp":1671566928197,"user_tz":480,"elapsed":11703,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"b986bd0c-1f52-4108-acce-94b9d4a92e9c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["targets.shape: (622400,)\n","contexts.shape: (622400, 5)\n","labels.shape: (622400, 5)\n"]}]},{"cell_type":"code","source":["BATCH_SIZE = 1024\n","BUFFER_SIZE = 10000\n","dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","print(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QbXbi0NwMQ0D","executionInfo":{"status":"ok","timestamp":1671567670191,"user_tz":480,"elapsed":174,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"4c9e23bd-8934-4f28-b0bb-ec6d23b4222f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"]}]},{"cell_type":"markdown","source":["`dataset` is what we will use to train our word2vec model below."],"metadata":{"id":"1ki0L3X8PyQJ"}},{"cell_type":"markdown","source":["---------------------------------------------------"],"metadata":{"id":"2K0NSsiA_ZoL"}},{"cell_type":"code","source":["class Word2Vec(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim):\n","    super(Word2Vec, self).__init__()\n","    # each word, represented by an integer from 0 to vocab_size-1, is assigned a trainable vector of dimension `embedding_dim`=128. \n","    # That is what the following layers.Embedding thing is doing\n","    self.target_embedding = layers.Embedding(vocab_size,\n","                                      embedding_dim,\n","                                      input_length=1,\n","                                      name=\"w2v_embedding\")\n","    self.context_embedding = layers.Embedding(vocab_size,\n","                                       embedding_dim,\n","                                       input_length=num_ns+1)\n","\n","  def call(self, pair):\n","    target, context = pair\n","    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n","    # context: (batch, context)\n","    if len(target.shape) == 2:\n","      target = tf.squeeze(target, axis=1)\n","    # target: (batch,)\n","    word_emb = self.target_embedding(target)\n","    # word_emb: (batch, embed)\n","    context_emb = self.context_embedding(context)\n","    # context_emb: (batch, context, embed)\n","    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n","    # dots: (batch, context)\n","    return dots"],"metadata":{"id":"p2Aic0YWP6nZ","executionInfo":{"status":"ok","timestamp":1671571203604,"user_tz":480,"elapsed":153,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# try embedding layer\n","vocab_size_example=1000\n","emb_mod = tf.keras.Sequential()\n","emb_mod.add(tf.keras.layers.Embedding(input_dim=vocab_size_example, output_dim=128, input_length=1, name='my_emb'))\n","input_array = np.random.randint(vocab_size_example, size=(32, 1))\n","emb_mod.compile('rmsprop', 'mse')\n","output_array = emb_mod.predict(input_array)\n","output_array.shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1qsgEQbxVNz_","executionInfo":{"status":"ok","timestamp":1671605120172,"user_tz":480,"elapsed":266,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"720680a2-b9f5-4d23-e421-d8f8aa687c20"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 36ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["(32, 1, 128)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["wei = emb_mod.get_layer('my_emb').get_weights()\n","wei[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OoNTnGgAbgXW","executionInfo":{"status":"ok","timestamp":1671605191207,"user_tz":480,"elapsed":262,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"a4502d88-f32a-4437-b711-e0c786a4e612"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 128)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["embedding_dim = 128\n","word2vec = Word2Vec(vocab_size=len(my_vocab), embedding_dim=embedding_dim)\n","word2vec.compile(optimizer='adam',\n","                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                 metrics=['accuracy'])"],"metadata":{"id":"4kAzzC8XaCJ1","executionInfo":{"status":"ok","timestamp":1671571518736,"user_tz":480,"elapsed":168,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["fit the model:"],"metadata":{"id":"lLR4m-7cbdIm"}},{"cell_type":"code","source":["\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=CHECKPOINT_PATH, \n","    verbose=1,\n","    save_freq = 'epoch')\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","word2vec.fit(dataset, epochs=20, callbacks=[cp_callback, tensorboard_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"spVMDhprccnq","executionInfo":{"status":"ok","timestamp":1671572555999,"user_tz":480,"elapsed":382361,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"3a35bb1e-a439-4f3a-cfa2-d3f6faf9ca5d"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","606/607 [============================>.] - ETA: 0s - loss: 1.0013 - accuracy: 0.6429\n","Epoch 1: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 22s 34ms/step - loss: 1.0010 - accuracy: 0.6430\n","Epoch 2/20\n","606/607 [============================>.] - ETA: 0s - loss: 0.7726 - accuracy: 0.7031\n","Epoch 2: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 16s 26ms/step - loss: 0.7726 - accuracy: 0.7031\n","Epoch 3/20\n","607/607 [==============================] - ETA: 0s - loss: 0.7217 - accuracy: 0.7215\n","Epoch 3: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 16s 26ms/step - loss: 0.7217 - accuracy: 0.7215\n","Epoch 4/20\n","607/607 [==============================] - ETA: 0s - loss: 0.6831 - accuracy: 0.7374\n","Epoch 4: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 16s 27ms/step - loss: 0.6831 - accuracy: 0.7374\n","Epoch 5/20\n","607/607 [==============================] - ETA: 0s - loss: 0.6482 - accuracy: 0.7526\n","Epoch 5: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 16s 26ms/step - loss: 0.6482 - accuracy: 0.7526\n","Epoch 6/20\n","607/607 [==============================] - ETA: 0s - loss: 0.6154 - accuracy: 0.7668\n","Epoch 6: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 15s 26ms/step - loss: 0.6154 - accuracy: 0.7668\n","Epoch 7/20\n","606/607 [============================>.] - ETA: 0s - loss: 0.5849 - accuracy: 0.7800\n","Epoch 7: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 17s 28ms/step - loss: 0.5849 - accuracy: 0.7800\n","Epoch 8/20\n","605/607 [============================>.] - ETA: 0s - loss: 0.5569 - accuracy: 0.7923\n","Epoch 8: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 15s 25ms/step - loss: 0.5568 - accuracy: 0.7923\n","Epoch 9/20\n","607/607 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.8035\n","Epoch 9: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 15s 25ms/step - loss: 0.5313 - accuracy: 0.8035\n","Epoch 10/20\n","606/607 [============================>.] - ETA: 0s - loss: 0.5082 - accuracy: 0.8136\n","Epoch 10: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 15s 25ms/step - loss: 0.5082 - accuracy: 0.8136\n","Epoch 11/20\n","605/607 [============================>.] - ETA: 0s - loss: 0.4875 - accuracy: 0.8222\n","Epoch 11: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 15s 25ms/step - loss: 0.4874 - accuracy: 0.8222\n","Epoch 12/20\n","607/607 [==============================] - ETA: 0s - loss: 0.4689 - accuracy: 0.8295\n","Epoch 12: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 16s 26ms/step - loss: 0.4689 - accuracy: 0.8295\n","Epoch 13/20\n","606/607 [============================>.] - ETA: 0s - loss: 0.4525 - accuracy: 0.8360\n","Epoch 13: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 15s 25ms/step - loss: 0.4525 - accuracy: 0.8360\n","Epoch 14/20\n","606/607 [============================>.] - ETA: 0s - loss: 0.4379 - accuracy: 0.8414\n","Epoch 14: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 15s 25ms/step - loss: 0.4378 - accuracy: 0.8414\n","Epoch 15/20\n","607/607 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.8461\n","Epoch 15: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 16s 26ms/step - loss: 0.4249 - accuracy: 0.8461\n","Epoch 16/20\n","607/607 [==============================] - ETA: 0s - loss: 0.4134 - accuracy: 0.8502\n","Epoch 16: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 17s 28ms/step - loss: 0.4134 - accuracy: 0.8502\n","Epoch 17/20\n","605/607 [============================>.] - ETA: 0s - loss: 0.4032 - accuracy: 0.8539\n","Epoch 17: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 15s 25ms/step - loss: 0.4032 - accuracy: 0.8539\n","Epoch 18/20\n","606/607 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8569\n","Epoch 18: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 16s 27ms/step - loss: 0.3941 - accuracy: 0.8569\n","Epoch 19/20\n","606/607 [============================>.] - ETA: 0s - loss: 0.3860 - accuracy: 0.8596\n","Epoch 19: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 17s 27ms/step - loss: 0.3860 - accuracy: 0.8596\n","Epoch 20/20\n","607/607 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.8619\n","Epoch 20: saving model to /content/gdrive/My Drive/Colab/checkpoints/word2vec.ckpt\n","607/607 [==============================] - 15s 25ms/step - loss: 0.3787 - accuracy: 0.8619\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fb687751850>"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["---------------------------------------------------------------------------------------------------------"],"metadata":{"id":"xLzIz1tdP9X2"}},{"cell_type":"markdown","source":["Check if can reload model from drive:"],"metadata":{"id":"ncLA1164g4i-"}},{"cell_type":"code","source":["loaded_model = tf.keras.models.load_model(CHECKPOINT_PATH)\n"],"metadata":{"id":"gK8gE1dqg8PX","executionInfo":{"status":"ok","timestamp":1671605025872,"user_tz":480,"elapsed":7534,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Obtain the weights from the model using Model.get_layer and Layer.get_weights. \n","\n","weights = loaded_model.get_layer('w2v_embedding').get_weights()[0]\n","weights.shape # indeed, len(my_vocab) is also 7328"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WykuTz2MirVQ","executionInfo":{"status":"ok","timestamp":1671605372164,"user_tz":480,"elapsed":303,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"700687c4-8b26-4c4c-f8d6-042d9ff8cb90"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7328, 128)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Given a word, show the closest words to this word in terms of the vector representation of the model:"],"metadata":{"id":"1Zn1PQdrhfgZ"}},{"cell_type":"code","source":["ind = my_vocab['emma']\n","dist = np.ones(len(my_vocab))*(-1) # stores the euclidean distance between each word vector and the word vector specified by index `ind`\n","for i in range(len(my_vocab)):\n","  dist[i] = np.linalg.norm(weights[i] - weights[ind]) # euclidean distance\n"],"metadata":{"id":"qRJySMa3dIPW","executionInfo":{"status":"ok","timestamp":1671606784054,"user_tz":480,"elapsed":284,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["sorted_indices = np.argsort(dist)\n","print([my_inverse_vocab[sorted_indices[i]] for i in range(20)]) # the closest words shown here don't seem to be semantically close???"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQc2Ys_6fp86","executionInfo":{"status":"ok","timestamp":1671606785950,"user_tz":480,"elapsed":3,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"978e3ab1-1baf-430c-8734-456195c2e368"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["['emma', 'meditations', 'middling', 'repeat', 'brewing', 'assert', 'ungraciously', 'sanguinely', 'grammatical', 'softly', '_bride_', 'sacred', 'dirt', 'exult', 'conditionally', 'alas', 'fight', 'unexpensively', 'begging', '_court_']\n"]}]},{"cell_type":"markdown","source":["-----------------------------------------------------------------------------------------------------"],"metadata":{"id":"QmnabkxBib-D"}},{"cell_type":"code","source":["window_size = 2\n","my_skipgrams, labels = tf.keras.preprocessing.sequence.skipgrams(\n","      example_sequence,\n","      vocabulary_size=vocab_size,\n","      window_size=window_size,\n","      negative_samples=4.0)\n","my_skipgrams = np.array(my_skipgrams); labels = np.array(labels)\n","my_pos_skipgrams = my_skipgrams[labels==1]\n","my_neg_skipgrams = my_skipgrams[labels==0]"],"metadata":{"id":"T1MJkEA_Q31N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","labels_mask = np.array([bool(labels[i]) for i in range(len(labels))])\n","labels_mask_negated = np.array([bool(1-labels[i]) for i in range(len(labels))])\n","my_pos_skipgrams = tf.boolean_mask(my_skipgrams, labels_mask)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0q39lTAMSN3K","executionInfo":{"status":"ok","timestamp":1671476909279,"user_tz":480,"elapsed":132,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"94a738d3-a362-4a78-ac73-4cfe8387c94b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(26, 2), dtype=int32, numpy=\n","array([[4, 5],\n","       [6, 1],\n","       [7, 6],\n","       [1, 4],\n","       [5, 3],\n","       [1, 5],\n","       [4, 1],\n","       [3, 2],\n","       [3, 4],\n","       [6, 7],\n","       [1, 3],\n","       [5, 4],\n","       [6, 5],\n","       [7, 1],\n","       [2, 1],\n","       [1, 7],\n","       [1, 6],\n","       [4, 3],\n","       [1, 2],\n","       [5, 6],\n","       [3, 5],\n","       [2, 3],\n","       [5, 1],\n","       [2, 4],\n","       [3, 1],\n","       [4, 2]], dtype=int32)>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["window_size = 2\n","positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","      example_sequence,\n","      vocabulary_size=vocab_size,\n","      window_size=window_size,\n","      negative_samples=0)\n","\n","\n","for target, context in positive_skip_grams:\n","  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\n","# WHY IS THERE (1,5)? ISN'T THE WINDOW SIZE 2?"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKHsBC05I78L","executionInfo":{"status":"ok","timestamp":1671434309121,"user_tz":480,"elapsed":118,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"c3bf465b-1365-4079-bfa2-4ebaefc2781e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3, 2): (road, wide)\n","(4, 3): (shimmered, road)\n","(4, 1): (shimmered, the)\n","(6, 7): (hot, sun)\n","(1, 2): (the, wide)\n","(5, 4): (in, shimmered)\n","(7, 1): (sun, the)\n","(3, 5): (road, in)\n","(2, 3): (wide, road)\n","(1, 5): (the, in)\n","(3, 1): (road, the)\n","(5, 1): (in, the)\n","(6, 1): (hot, the)\n","(2, 4): (wide, shimmered)\n","(4, 2): (shimmered, wide)\n","(7, 6): (sun, hot)\n","(1, 6): (the, hot)\n","(5, 3): (in, road)\n","(2, 1): (wide, the)\n","(1, 3): (the, road)\n","(1, 7): (the, sun)\n","(3, 4): (road, shimmered)\n","(4, 5): (shimmered, in)\n","(5, 6): (in, hot)\n","(1, 4): (the, shimmered)\n","(6, 5): (hot, in)\n"]}]},{"cell_type":"code","source":["# Get target and context words for one positive skip-gram.\n","target_word, context_word = positive_skip_grams[0]\n","print(inverse_vocab[target_word])\n","print(inverse_vocab[context_word])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMMVy37NLYdI","executionInfo":{"status":"ok","timestamp":1671433240871,"user_tz":480,"elapsed":105,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"d795721c-b94c-48a6-ed71-fd4d40059b09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["shimmered\n","road\n"]}]},{"cell_type":"code","source":["# NEGATIVE SAMPLES ARE WEIRD\n","\n","# Set the number of negative samples per positive context.\n","num_ns = 4\n","\n","context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n","negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","    true_classes=context_class,  # class that should be sampled as 'positive'\n","    num_true=1,  # each positive skip-gram has 1 positive context class\n","    num_sampled=num_ns,  # number of negative context words to sample\n","    unique=True,  # all the negative samples should be unique\n","    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n","    seed=SEED,  # seed for reproducibility\n","    name=\"negative_sampling\"  # name of this operation\n",")\n","print(negative_sampling_candidates)\n","print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates]) # why is there still \"shimmered\" or \"road\" in my negative sample???"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJ2_IyPELPX2","executionInfo":{"status":"ok","timestamp":1671433335485,"user_tz":480,"elapsed":107,"user":{"displayName":"許瀚元","userId":"06242862820898425544"}},"outputId":"813d531a-78df-44fd-a8a0-008d8d41a887"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([2 0 3 1], shape=(4,), dtype=int64)\n","['wide', '<pad>', 'road', 'the']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ivfiu6DQMNNr"},"execution_count":null,"outputs":[]}]}